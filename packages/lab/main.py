import ollama
import json

# --- 1. Fun√ß√µes Python ---


def get_weather(city: str):
    """Fun√ß√£o Python que busca o clima"""
    print(f"--- Executando a ferramenta get_weather(city={city}) ---")
    if "s√£o paulo" in city.lower():
        return json.dumps({"temperature": "25¬∞C", "condition": "ensolarado"})
    elif "londres" in city.lower():
        return json.dumps({"temperature": "15¬∞C", "condition": "nublado"})
    else:
        return json.dumps({"temperature": "N/A", "condition": "cidade n√£o encontrada"})


def get_stock_price(symbol: str):
    """Fun√ß√£o Python que busca o pre√ßo de a√ß√µes"""
    symbol = symbol.upper()
    print(f"--- Executando a ferramenta get_stock_price(symbol={symbol}) ---")
    if symbol == "PETR4":
        return json.dumps({"symbol": "PETR4", "price": 38.50, "currency": "BRL"})
    elif symbol == "AAPL":
        return json.dumps({"symbol": "AAPL", "price": 215.00, "currency": "USD"})
    else:
        return json.dumps(
            {"symbol": symbol, "price": "N/A", "error": "s√≠mbolo n√£o encontrado"}
        )


# --- 2. Ferramentas para o Modelo ---
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Obt√©m o clima atual para uma cidade espec√≠fica.",
            "parameters": {
                "type": "object",
                "properties": {
                    "city": {
                        "type": "string",
                        "description": "O nome da cidade, ex: S√£o Paulo, Londres",
                    }
                },
                "required": ["city"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "get_stock_price",
            "description": "Obt√©m o pre√ßo de uma a√ß√£o (stock) pelo seu s√≠mbolo (ticker).",
            "parameters": {
                "type": "object",
                "properties": {
                    "symbol": {
                        "type": "string",
                        "description": "O s√≠mbolo (ticker) da a√ß√£o, ex: PETR4, AAPL",
                    }
                },
                "required": ["symbol"],
            },
        },
    },
]

# --- 3. Loop de Chat Interativo ---
print("Iniciando chat com 'Tool Use'. Digite 'sair' para terminar.")
while True:
    # qwen3:4b, llama3:8b
    MODELO = "qwen3:4b"

    prompt = input("\nVoc√™: ")
    if prompt.lower() == "sair":
        break

    # Zera o hist√≥rico de mensagens a cada novo prompt
    messages = [{"role": "user", "content": prompt}]

    print(f"...")

    # --- 4. Primeira chamada ao modelo (com as ferramentas) ---
    try:
        response = ollama.chat(
            model=MODELO,
            messages=messages,
            tools=tools,
        )
    except Exception as e:
        print(f"Erro ao chamar o modelo: {e}")
        continue

    # Adiciona a resposta (que deve ser um pedido de ferramenta) ao hist√≥rico
    messages.append(response["message"])

    # --- 5. Verifique se o modelo pediu para usar uma ferramenta ---
    if not response["message"].get("tool_calls"):
        print("\n[RESPOSTA DIRETA DO MODELO] ü§ñ")
        print(response["message"]["content"])
        continue  # O modelo respondeu diretamente, sem usar ferramenta

    # --- 6. Modelo Pediu uma Ferramenta ---
    print("\n[MODELO PEDINDO FERRAMENTA] üß†")
    tool_calls = response["message"]["tool_calls"]

    # Loop para o caso do modelo querer chamar v√°rias ferramentas
    for call in tool_calls:
        function_name = call["function"]["name"]
        function_args = call["function"]["arguments"]

        print(f"O modelo quer chamar: {function_name}({json.dumps(function_args)})")

        # --- 7. Execute a ferramenta "real" ---
        if function_name == "get_weather":
            city = function_args.get("city")
            result = get_weather(city)

        elif function_name == "get_stock_price":
            symbol = function_args.get("symbol")
            result = get_stock_price(symbol)

        else:
            print(f"Erro: Modelo tentou chamar fun√ß√£o desconhecida: {function_name}")
            result = json.dumps({"error": "fun√ß√£o n√£o implementada"})

        # --- 8. Prepare a resposta da ferramenta ---
        print("\n[RESPOSTA DA FERRAMENTA] üõ†Ô∏è")
        print(f"A fun√ß√£o retornou (raw JSON): {result}")

        # Adicione a resposta da ferramenta ao hist√≥rico para o modelo "ler"
        messages.append(
            {
                "role": "tool",
                "content": result,
                # "name": function_name # Opcional, mas recomendado
            }
        )

        # --- 9. Chame o modelo uma √öLTIMA vez ---
        # Agora o modelo tem o contexto (prompt + pedido + resultado)
        # para gerar a resposta final em linguagem natural.

        print("...")

        final_response = ollama.chat(model=MODELO, messages=messages)

        print("\n[RESPOSTA FINAL DO MODELO] ü§ñ")
        print(final_response["message"]["content"])
